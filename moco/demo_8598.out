Dataset is copied to /tmp
Use GPU: 0 for training
=> creating model 'resnet50'
MoCo(
  (encoder_q): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Sequential(
      (0): Linear(in_features=2048, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=128, bias=True)
    )
  )
  (encoder_k): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Sequential(
      (0): Linear(in_features=2048, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=128, bias=True)
    )
  )
)
Epoch: [0][   0/1000]	Time 10.666 (10.666)	Data  2.822 ( 2.822)	Loss 6.8251e+00 (6.8251e+00)	Acc@1 100.00 (100.00)	Acc@5 100.00 (100.00)
Epoch: [0][ 500/1000]	Time  0.763 ( 0.775)	Data  0.000 ( 0.008)	Loss 1.0120e+01 (1.0061e+01)	Acc@1   0.39 (  1.11)	Acc@5   1.95 (  3.96)
Epoch: [1][   0/1000]	Time  2.846 ( 2.846)	Data  2.233 ( 2.233)	Loss 1.0127e+01 (1.0127e+01)	Acc@1   0.39 (  0.39)	Acc@5   1.95 (  1.95)
Epoch: [1][ 500/1000]	Time  0.765 ( 0.762)	Data  0.000 ( 0.005)	Loss 1.0101e+01 (1.0123e+01)	Acc@1   0.78 (  1.05)	Acc@5   4.30 (  3.50)
Epoch: [2][   0/1000]	Time  2.812 ( 2.812)	Data  2.179 ( 2.179)	Loss 1.0354e+01 (1.0354e+01)	Acc@1   0.00 (  0.00)	Acc@5   1.95 (  1.95)
Epoch: [2][ 500/1000]	Time  0.753 ( 0.760)	Data  0.000 ( 0.005)	Loss 1.0460e+01 (1.0435e+01)	Acc@1   0.78 (  0.86)	Acc@5   2.34 (  2.66)
Epoch: [3][   0/1000]	Time  2.942 ( 2.942)	Data  2.329 ( 2.329)	Loss 1.0418e+01 (1.0418e+01)	Acc@1   0.39 (  0.39)	Acc@5   1.17 (  1.17)
Epoch: [3][ 500/1000]	Time  0.767 ( 0.761)	Data  0.000 ( 0.005)	Loss 1.0417e+01 (1.0427e+01)	Acc@1   0.00 (  0.18)	Acc@5   0.00 (  0.69)
Epoch: [4][   0/1000]	Time  2.931 ( 2.931)	Data  2.286 ( 2.286)	Loss 1.0366e+01 (1.0366e+01)	Acc@1   0.39 (  0.39)	Acc@5   0.39 (  0.39)
Epoch: [4][ 500/1000]	Time  0.758 ( 0.761)	Data  0.000 ( 0.005)	Loss 9.9964e+00 (1.0159e+01)	Acc@1   0.39 (  0.26)	Acc@5   0.39 (  0.93)
Epoch: [5][   0/1000]	Time  2.929 ( 2.929)	Data  2.314 ( 2.314)	Loss 9.8295e+00 (9.8295e+00)	Acc@1   0.39 (  0.39)	Acc@5   0.39 (  0.39)
Epoch: [5][ 500/1000]	Time  0.756 ( 0.761)	Data  0.000 ( 0.005)	Loss 9.7376e+00 (9.7677e+00)	Acc@1   0.00 (  0.39)	Acc@5   0.39 (  1.36)
Epoch: [6][   0/1000]	Time  2.868 ( 2.868)	Data  2.264 ( 2.264)	Loss 9.4888e+00 (9.4888e+00)	Acc@1   0.39 (  0.39)	Acc@5   1.56 (  1.56)
Epoch: [6][ 500/1000]	Time  0.754 ( 0.761)	Data  0.000 ( 0.005)	Loss 9.4618e+00 (9.4623e+00)	Acc@1   0.39 (  0.46)	Acc@5   1.95 (  1.58)
Epoch: [7][   0/1000]	Time  2.904 ( 2.904)	Data  2.239 ( 2.239)	Loss 9.1958e+00 (9.1958e+00)	Acc@1   0.39 (  0.39)	Acc@5   2.34 (  2.34)
Epoch: [7][ 500/1000]	Time  0.750 ( 0.760)	Data  0.000 ( 0.005)	Loss 9.1674e+00 (9.2014e+00)	Acc@1   0.78 (  0.65)	Acc@5   1.95 (  2.26)
Epoch: [8][   0/1000]	Time  2.891 ( 2.891)	Data  2.226 ( 2.226)	Loss 9.0379e+00 (9.0379e+00)	Acc@1   1.95 (  1.95)	Acc@5   5.86 (  5.86)
Epoch: [8][ 500/1000]	Time  0.753 ( 0.760)	Data  0.000 ( 0.005)	Loss 8.9893e+00 (8.9991e+00)	Acc@1   2.34 (  1.07)	Acc@5   6.64 (  3.59)
Epoch: [9][   0/1000]	Time  2.837 ( 2.837)	Data  2.214 ( 2.214)	Loss 8.8524e+00 (8.8524e+00)	Acc@1   2.73 (  2.73)	Acc@5   7.03 (  7.03)
Epoch: [9][ 500/1000]	Time  0.761 ( 0.761)	Data  0.000 ( 0.005)	Loss 8.8414e+00 (8.8382e+00)	Acc@1   1.56 (  2.17)	Acc@5   4.69 (  6.34)
Epoch: [10][   0/1000]	Time  2.890 ( 2.890)	Data  2.243 ( 2.243)	Loss 8.7268e+00 (8.7268e+00)	Acc@1   1.56 (  1.56)	Acc@5   6.64 (  6.64)
Epoch: [10][ 500/1000]	Time  0.755 ( 0.762)	Data  0.000 ( 0.005)	Loss 8.6534e+00 (8.6928e+00)	Acc@1   2.73 (  3.26)	Acc@5   7.81 (  9.16)
Epoch: [11][   0/1000]	Time  2.750 ( 2.750)	Data  2.126 ( 2.126)	Loss 8.6409e+00 (8.6409e+00)	Acc@1   2.73 (  2.73)	Acc@5  11.72 ( 11.72)
Epoch: [11][ 500/1000]	Time  0.755 ( 0.760)	Data  0.000 ( 0.005)	Loss 8.6157e+00 (8.5693e+00)	Acc@1   2.73 (  4.52)	Acc@5  12.11 ( 11.79)
Epoch: [12][   0/1000]	Time  2.957 ( 2.957)	Data  2.324 ( 2.324)	Loss 8.4577e+00 (8.4577e+00)	Acc@1   4.69 (  4.69)	Acc@5  13.67 ( 13.67)
Epoch: [12][ 500/1000]	Time  0.761 ( 0.763)	Data  0.000 ( 0.005)	Loss 8.3866e+00 (8.4792e+00)	Acc@1   4.69 (  5.95)	Acc@5  19.92 ( 14.32)
Epoch: [13][   0/1000]	Time  2.907 ( 2.907)	Data  2.288 ( 2.288)	Loss 8.4626e+00 (8.4626e+00)	Acc@1   6.25 (  6.25)	Acc@5  16.80 ( 16.80)
Epoch: [13][ 500/1000]	Time  0.763 ( 0.761)	Data  0.000 ( 0.005)	Loss 8.4298e+00 (8.4016e+00)	Acc@1   8.20 (  7.24)	Acc@5  14.84 ( 16.88)
Epoch: [14][   0/1000]	Time  2.880 ( 2.880)	Data  2.260 ( 2.260)	Loss 8.3958e+00 (8.3958e+00)	Acc@1   9.38 (  9.38)	Acc@5  17.58 ( 17.58)
Epoch: [14][ 500/1000]	Time  0.755 ( 0.762)	Data  0.000 ( 0.005)	Loss 8.3105e+00 (8.3203e+00)	Acc@1   8.59 (  9.08)	Acc@5  23.05 ( 19.87)
Epoch: [15][   0/1000]	Time  2.957 ( 2.957)	Data  2.332 ( 2.332)	Loss 8.2420e+00 (8.2420e+00)	Acc@1  10.16 ( 10.16)	Acc@5  19.92 ( 19.92)
Epoch: [15][ 500/1000]	Time  0.765 ( 0.763)	Data  0.000 ( 0.005)	Loss 8.3035e+00 (8.2465e+00)	Acc@1  10.94 ( 10.94)	Acc@5  21.88 ( 23.00)
Epoch: [16][   0/1000]	Time  2.857 ( 2.857)	Data  2.233 ( 2.233)	Loss 8.2789e+00 (8.2789e+00)	Acc@1  14.06 ( 14.06)	Acc@5  22.27 ( 22.27)
Epoch: [16][ 500/1000]	Time  0.762 ( 0.764)	Data  0.000 ( 0.005)	Loss 8.2309e+00 (8.1722e+00)	Acc@1  14.06 ( 13.06)	Acc@5  24.22 ( 26.07)
Epoch: [17][   0/1000]	Time  2.818 ( 2.818)	Data  2.178 ( 2.178)	Loss 8.0861e+00 (8.0861e+00)	Acc@1  10.16 ( 10.16)	Acc@5  29.30 ( 29.30)
Epoch: [17][ 500/1000]	Time  0.760 ( 0.762)	Data  0.000 ( 0.005)	Loss 8.0453e+00 (8.1020e+00)	Acc@1  17.19 ( 15.26)	Acc@5  28.91 ( 29.09)
Epoch: [18][   0/1000]	Time  2.910 ( 2.910)	Data  2.290 ( 2.290)	Loss 8.1018e+00 (8.1018e+00)	Acc@1  17.19 ( 17.19)	Acc@5  29.30 ( 29.30)
Epoch: [18][ 500/1000]	Time  0.767 ( 0.763)	Data  0.000 ( 0.005)	Loss 8.0463e+00 (8.0383e+00)	Acc@1  17.97 ( 17.73)	Acc@5  33.20 ( 32.34)
Epoch: [19][   0/1000]	Time  2.847 ( 2.847)	Data  2.226 ( 2.226)	Loss 8.0259e+00 (8.0259e+00)	Acc@1  17.58 ( 17.58)	Acc@5  33.20 ( 33.20)
Epoch: [19][ 500/1000]	Time  0.757 ( 0.763)	Data  0.000 ( 0.005)	Loss 7.9471e+00 (7.9771e+00)	Acc@1  18.75 ( 20.32)	Acc@5  33.20 ( 35.75)
Epoch: [20][   0/1000]	Time  2.924 ( 2.924)	Data  2.298 ( 2.298)	Loss 7.9771e+00 (7.9771e+00)	Acc@1  23.05 ( 23.05)	Acc@5  34.77 ( 34.77)
Epoch: [20][ 500/1000]	Time  0.752 ( 0.763)	Data  0.000 ( 0.005)	Loss 7.8767e+00 (7.9250e+00)	Acc@1  27.34 ( 22.49)	Acc@5  45.70 ( 38.38)
Epoch: [21][   0/1000]	Time  2.746 ( 2.746)	Data  2.148 ( 2.148)	Loss 7.8710e+00 (7.8710e+00)	Acc@1  22.27 ( 22.27)	Acc@5  38.67 ( 38.67)
Epoch: [21][ 500/1000]	Time  0.768 ( 0.763)	Data  0.000 ( 0.005)	Loss 7.8816e+00 (7.8789e+00)	Acc@1  25.78 ( 24.76)	Acc@5  41.02 ( 41.09)
Epoch: [22][   0/1000]	Time  2.854 ( 2.854)	Data  2.214 ( 2.214)	Loss 7.9283e+00 (7.9283e+00)	Acc@1  23.05 ( 23.05)	Acc@5  42.19 ( 42.19)
Epoch: [22][ 500/1000]	Time  0.767 ( 0.762)	Data  0.000 ( 0.005)	Loss 7.8329e+00 (7.8368e+00)	Acc@1  24.61 ( 27.07)	Acc@5  41.41 ( 43.64)
Epoch: [23][   0/1000]	Time  2.944 ( 2.944)	Data  2.282 ( 2.282)	Loss 7.7641e+00 (7.7641e+00)	Acc@1  31.25 ( 31.25)	Acc@5  47.66 ( 47.66)
Epoch: [23][ 500/1000]	Time  0.757 ( 0.762)	Data  0.000 ( 0.005)	Loss 7.7574e+00 (7.7918e+00)	Acc@1  31.64 ( 29.33)	Acc@5  44.14 ( 46.10)
Epoch: [24][   0/1000]	Time  2.913 ( 2.913)	Data  2.229 ( 2.229)	Loss 7.7816e+00 (7.7816e+00)	Acc@1  26.95 ( 26.95)	Acc@5  45.31 ( 45.31)
Epoch: [24][ 500/1000]	Time  0.754 ( 0.763)	Data  0.000 ( 0.005)	Loss 7.7383e+00 (7.7594e+00)	Acc@1  30.08 ( 31.39)	Acc@5  48.83 ( 48.23)
Epoch: [25][   0/1000]	Time  2.844 ( 2.844)	Data  2.204 ( 2.204)	Loss 7.7569e+00 (7.7569e+00)	Acc@1  33.20 ( 33.20)	Acc@5  46.09 ( 46.09)
Epoch: [25][ 500/1000]	Time  0.759 ( 0.761)	Data  0.000 ( 0.005)	Loss 7.7369e+00 (7.7234e+00)	Acc@1  31.64 ( 33.37)	Acc@5  50.39 ( 50.65)
Epoch: [26][   0/1000]	Time  2.785 ( 2.785)	Data  2.150 ( 2.150)	Loss 7.6201e+00 (7.6201e+00)	Acc@1  36.33 ( 36.33)	Acc@5  55.08 ( 55.08)
Epoch: [26][ 500/1000]	Time  0.751 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.6858e+00 (7.6928e+00)	Acc@1  37.50 ( 35.19)	Acc@5  49.22 ( 52.40)
Epoch: [27][   0/1000]	Time  2.820 ( 2.820)	Data  2.129 ( 2.129)	Loss 7.6696e+00 (7.6696e+00)	Acc@1  32.81 ( 32.81)	Acc@5  52.34 ( 52.34)
Epoch: [27][ 500/1000]	Time  0.750 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.7183e+00 (7.6647e+00)	Acc@1  39.45 ( 36.83)	Acc@5  57.03 ( 54.23)
Epoch: [28][   0/1000]	Time  2.826 ( 2.826)	Data  2.212 ( 2.212)	Loss 7.6518e+00 (7.6518e+00)	Acc@1  33.98 ( 33.98)	Acc@5  56.64 ( 56.64)
Epoch: [28][ 500/1000]	Time  0.761 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.6942e+00 (7.6376e+00)	Acc@1  35.16 ( 38.90)	Acc@5  53.12 ( 56.10)
Epoch: [29][   0/1000]	Time  2.912 ( 2.912)	Data  2.262 ( 2.262)	Loss 7.6207e+00 (7.6207e+00)	Acc@1  39.06 ( 39.06)	Acc@5  57.42 ( 57.42)
Epoch: [29][ 500/1000]	Time  0.756 ( 0.761)	Data  0.000 ( 0.005)	Loss 7.6297e+00 (7.6096e+00)	Acc@1  40.23 ( 40.76)	Acc@5  56.64 ( 57.85)
Epoch: [30][   0/1000]	Time  2.914 ( 2.914)	Data  2.314 ( 2.314)	Loss 7.6624e+00 (7.6624e+00)	Acc@1  35.94 ( 35.94)	Acc@5  55.86 ( 55.86)
Epoch: [30][ 500/1000]	Time  0.756 ( 0.760)	Data  0.000 ( 0.005)	Loss 7.5192e+00 (7.5819e+00)	Acc@1  46.09 ( 42.45)	Acc@5  63.67 ( 59.51)
Epoch: [31][   0/1000]	Time  2.791 ( 2.791)	Data  2.165 ( 2.165)	Loss 7.5199e+00 (7.5199e+00)	Acc@1  42.97 ( 42.97)	Acc@5  64.45 ( 64.45)
Epoch: [31][ 500/1000]	Time  0.757 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.5159e+00 (7.5590e+00)	Acc@1  45.70 ( 44.20)	Acc@5  64.06 ( 61.23)
Epoch: [32][   0/1000]	Time  2.884 ( 2.884)	Data  2.228 ( 2.228)	Loss 7.5704e+00 (7.5704e+00)	Acc@1  44.92 ( 44.92)	Acc@5  64.06 ( 64.06)
Epoch: [32][ 500/1000]	Time  0.753 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.5401e+00 (7.5365e+00)	Acc@1  45.31 ( 45.52)	Acc@5  64.45 ( 62.56)
Epoch: [33][   0/1000]	Time  2.804 ( 2.804)	Data  2.191 ( 2.191)	Loss 7.4699e+00 (7.4699e+00)	Acc@1  49.22 ( 49.22)	Acc@5  65.23 ( 65.23)
Epoch: [33][ 500/1000]	Time  0.754 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.5422e+00 (7.5173e+00)	Acc@1  47.27 ( 47.20)	Acc@5  64.06 ( 63.80)
Epoch: [34][   0/1000]	Time  2.920 ( 2.920)	Data  2.309 ( 2.309)	Loss 7.4997e+00 (7.4997e+00)	Acc@1  45.70 ( 45.70)	Acc@5  65.23 ( 65.23)
Epoch: [34][ 500/1000]	Time  0.750 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.4529e+00 (7.4956e+00)	Acc@1  49.61 ( 48.46)	Acc@5  63.67 ( 65.26)
Epoch: [35][   0/1000]	Time  2.840 ( 2.840)	Data  2.231 ( 2.231)	Loss 7.4625e+00 (7.4625e+00)	Acc@1  48.44 ( 48.44)	Acc@5  67.58 ( 67.58)
Epoch: [35][ 500/1000]	Time  0.752 ( 0.758)	Data  0.000 ( 0.005)	Loss 7.4457e+00 (7.4764e+00)	Acc@1  51.17 ( 49.70)	Acc@5  66.41 ( 66.30)
Epoch: [36][   0/1000]	Time  2.848 ( 2.848)	Data  2.238 ( 2.238)	Loss 7.4342e+00 (7.4342e+00)	Acc@1  49.61 ( 49.61)	Acc@5  66.41 ( 66.41)
Epoch: [36][ 500/1000]	Time  0.754 ( 0.758)	Data  0.000 ( 0.005)	Loss 7.4619e+00 (7.4610e+00)	Acc@1  51.17 ( 50.69)	Acc@5  66.80 ( 67.13)
Epoch: [37][   0/1000]	Time  2.866 ( 2.866)	Data  2.227 ( 2.227)	Loss 7.4375e+00 (7.4375e+00)	Acc@1  51.95 ( 51.95)	Acc@5  69.92 ( 69.92)
Epoch: [37][ 500/1000]	Time  0.750 ( 0.759)	Data  0.000 ( 0.005)	Loss 7.4153e+00 (7.4429e+00)	Acc@1  53.12 ( 52.01)	Acc@5  70.70 ( 68.30)
Epoch: [38][   0/1000]	Time  2.864 ( 2.864)	Data  2.258 ( 2.258)	Loss 7.4742e+00 (7.4742e+00)	Acc@1  49.22 ( 49.22)	Acc@5  67.19 ( 67.19)
Epoch: [38][ 500/1000]	Time  0.752 ( 0.758)	Data  0.000 ( 0.005)	Loss 7.4465e+00 (7.4322e+00)	Acc@1  52.73 ( 52.93)	Acc@5  67.19 ( 68.92)
Epoch: [39][   0/1000]	Time  2.897 ( 2.897)	Data  2.230 ( 2.230)	Loss 7.4371e+00 (7.4371e+00)	Acc@1  53.12 ( 53.12)	Acc@5  69.14 ( 69.14)
Epoch: [39][ 500/1000]	Time  0.755 ( 0.758)	Data  0.000 ( 0.005)	Loss 7.3822e+00 (7.4104e+00)	Acc@1  53.52 ( 54.17)	Acc@5  71.09 ( 70.17)
Epoch: [40][   0/1000]	Time  2.889 ( 2.889)	Data  2.280 ( 2.280)	Loss 7.4893e+00 (7.4893e+00)	Acc@1  49.61 ( 49.61)	Acc@5  66.02 ( 66.02)
Epoch: [40][ 500/1000]	Time  0.747 ( 0.758)	Data  0.001 ( 0.005)	Loss 7.3674e+00 (7.4022e+00)	Acc@1  57.42 ( 54.75)	Acc@5  72.27 ( 70.46)
Epoch: [41][   0/1000]	Time  2.891 ( 2.891)	Data  2.284 ( 2.284)	Loss 7.3754e+00 (7.3754e+00)	Acc@1  49.22 ( 49.22)	Acc@5  69.53 ( 69.53)
Epoch: [41][ 500/1000]	Time  0.752 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3514e+00 (7.3908e+00)	Acc@1  55.47 ( 55.43)	Acc@5  71.88 ( 71.08)
Epoch: [42][   0/1000]	Time  2.850 ( 2.850)	Data  2.224 ( 2.224)	Loss 7.3068e+00 (7.3068e+00)	Acc@1  58.20 ( 58.20)	Acc@5  74.61 ( 74.61)
Epoch: [42][ 500/1000]	Time  0.753 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3548e+00 (7.3800e+00)	Acc@1  57.42 ( 56.19)	Acc@5  69.53 ( 71.83)
Epoch: [43][   0/1000]	Time  2.821 ( 2.821)	Data  2.195 ( 2.195)	Loss 7.4107e+00 (7.4107e+00)	Acc@1  53.12 ( 53.12)	Acc@5  72.27 ( 72.27)
Epoch: [43][ 500/1000]	Time  0.748 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3225e+00 (7.3698e+00)	Acc@1  61.33 ( 56.78)	Acc@5  75.78 ( 72.20)
Epoch: [44][   0/1000]	Time  2.847 ( 2.847)	Data  2.228 ( 2.228)	Loss 7.3814e+00 (7.3814e+00)	Acc@1  58.20 ( 58.20)	Acc@5  73.05 ( 73.05)
Epoch: [44][ 500/1000]	Time  0.753 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3787e+00 (7.3584e+00)	Acc@1  58.59 ( 57.63)	Acc@5  74.61 ( 72.88)
Epoch: [45][   0/1000]	Time  2.849 ( 2.849)	Data  2.198 ( 2.198)	Loss 7.3175e+00 (7.3175e+00)	Acc@1  57.81 ( 57.81)	Acc@5  75.00 ( 75.00)
Epoch: [45][ 500/1000]	Time  0.755 ( 0.758)	Data  0.000 ( 0.005)	Loss 7.3191e+00 (7.3501e+00)	Acc@1  59.38 ( 58.07)	Acc@5  74.61 ( 73.29)
Epoch: [46][   0/1000]	Time  2.878 ( 2.878)	Data  2.252 ( 2.252)	Loss 7.3363e+00 (7.3363e+00)	Acc@1  61.72 ( 61.72)	Acc@5  76.95 ( 76.95)
Epoch: [46][ 500/1000]	Time  0.764 ( 0.758)	Data  0.000 ( 0.005)	Loss 7.3479e+00 (7.3386e+00)	Acc@1  58.20 ( 58.76)	Acc@5  73.05 ( 73.72)
Epoch: [47][   0/1000]	Time  2.811 ( 2.811)	Data  2.187 ( 2.187)	Loss 7.3835e+00 (7.3835e+00)	Acc@1  51.56 ( 51.56)	Acc@5  67.19 ( 67.19)
Epoch: [47][ 500/1000]	Time  0.749 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.2696e+00 (7.3293e+00)	Acc@1  60.16 ( 59.46)	Acc@5  77.34 ( 74.28)
Epoch: [48][   0/1000]	Time  2.895 ( 2.895)	Data  2.241 ( 2.241)	Loss 7.3156e+00 (7.3156e+00)	Acc@1  59.77 ( 59.77)	Acc@5  76.56 ( 76.56)
Epoch: [48][ 500/1000]	Time  0.755 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3315e+00 (7.3202e+00)	Acc@1  60.55 ( 59.92)	Acc@5  75.78 ( 74.79)
Epoch: [49][   0/1000]	Time  2.890 ( 2.890)	Data  2.251 ( 2.251)	Loss 7.2640e+00 (7.2640e+00)	Acc@1  60.55 ( 60.55)	Acc@5  79.69 ( 79.69)
Epoch: [49][ 500/1000]	Time  0.750 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3032e+00 (7.3126e+00)	Acc@1  64.45 ( 60.47)	Acc@5  75.78 ( 75.22)
Epoch: [50][   0/1000]	Time  2.861 ( 2.861)	Data  2.231 ( 2.231)	Loss 7.3083e+00 (7.3083e+00)	Acc@1  57.42 ( 57.42)	Acc@5  76.95 ( 76.95)
Epoch: [50][ 500/1000]	Time  0.756 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.2775e+00 (7.3048e+00)	Acc@1  62.50 ( 60.89)	Acc@5  77.73 ( 75.47)
Epoch: [51][   0/1000]	Time  2.780 ( 2.780)	Data  2.159 ( 2.159)	Loss 7.3164e+00 (7.3164e+00)	Acc@1  57.42 ( 57.42)	Acc@5  70.31 ( 70.31)
Epoch: [51][ 500/1000]	Time  0.752 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.2774e+00 (7.2987e+00)	Acc@1  62.50 ( 61.19)	Acc@5  75.78 ( 75.74)
Epoch: [52][   0/1000]	Time  2.911 ( 2.911)	Data  2.254 ( 2.254)	Loss 7.3324e+00 (7.3324e+00)	Acc@1  58.98 ( 58.98)	Acc@5  72.66 ( 72.66)
Epoch: [52][ 500/1000]	Time  0.754 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.3098e+00 (7.2832e+00)	Acc@1  62.89 ( 62.12)	Acc@5  74.61 ( 76.65)
Epoch: [53][   0/1000]	Time  2.847 ( 2.847)	Data  2.246 ( 2.246)	Loss 7.2855e+00 (7.2855e+00)	Acc@1  60.16 ( 60.16)	Acc@5  75.00 ( 75.00)
Epoch: [53][ 500/1000]	Time  0.750 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2767e+00 (7.2782e+00)	Acc@1  60.55 ( 62.33)	Acc@5  78.12 ( 76.69)
Epoch: [54][   0/1000]	Time  2.857 ( 2.857)	Data  2.183 ( 2.183)	Loss 7.3368e+00 (7.3368e+00)	Acc@1  56.64 ( 56.64)	Acc@5  73.44 ( 73.44)
Epoch: [54][ 500/1000]	Time  0.751 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2375e+00 (7.2761e+00)	Acc@1  61.72 ( 62.46)	Acc@5  75.78 ( 76.76)
Epoch: [55][   0/1000]	Time  2.946 ( 2.946)	Data  2.325 ( 2.325)	Loss 7.2792e+00 (7.2792e+00)	Acc@1  60.16 ( 60.16)	Acc@5  75.00 ( 75.00)
Epoch: [55][ 500/1000]	Time  0.750 ( 0.757)	Data  0.000 ( 0.005)	Loss 7.2147e+00 (7.2679e+00)	Acc@1  64.06 ( 62.98)	Acc@5  76.56 ( 77.16)
Epoch: [56][   0/1000]	Time  2.786 ( 2.786)	Data  2.178 ( 2.178)	Loss 7.2930e+00 (7.2930e+00)	Acc@1  58.98 ( 58.98)	Acc@5  77.34 ( 77.34)
Epoch: [56][ 500/1000]	Time  0.753 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2426e+00 (7.2580e+00)	Acc@1  68.75 ( 63.48)	Acc@5  77.73 ( 77.54)
Epoch: [57][   0/1000]	Time  2.836 ( 2.836)	Data  2.212 ( 2.212)	Loss 7.2789e+00 (7.2789e+00)	Acc@1  59.38 ( 59.38)	Acc@5  77.73 ( 77.73)
Epoch: [57][ 500/1000]	Time  0.754 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2126e+00 (7.2568e+00)	Acc@1  65.23 ( 63.53)	Acc@5  83.20 ( 77.66)
Epoch: [58][   0/1000]	Time  2.899 ( 2.899)	Data  2.219 ( 2.219)	Loss 7.1515e+00 (7.1515e+00)	Acc@1  65.62 ( 65.62)	Acc@5  82.03 ( 82.03)
Epoch: [58][ 500/1000]	Time  0.746 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2473e+00 (7.2494e+00)	Acc@1  62.11 ( 63.98)	Acc@5  76.95 ( 77.86)
Epoch: [59][   0/1000]	Time  2.865 ( 2.865)	Data  2.254 ( 2.254)	Loss 7.2903e+00 (7.2903e+00)	Acc@1  57.81 ( 57.81)	Acc@5  74.61 ( 74.61)
Epoch: [59][ 500/1000]	Time  0.752 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2243e+00 (7.2405e+00)	Acc@1  67.58 ( 64.44)	Acc@5  78.91 ( 78.35)
Epoch: [60][   0/1000]	Time  2.838 ( 2.838)	Data  2.205 ( 2.205)	Loss 7.1494e+00 (7.1494e+00)	Acc@1  66.80 ( 66.80)	Acc@5  82.42 ( 82.42)
Epoch: [60][ 500/1000]	Time  0.755 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2333e+00 (7.2390e+00)	Acc@1  66.02 ( 64.54)	Acc@5  78.52 ( 78.27)
Epoch: [61][   0/1000]	Time  2.797 ( 2.797)	Data  2.181 ( 2.181)	Loss 7.2620e+00 (7.2620e+00)	Acc@1  62.50 ( 62.50)	Acc@5  77.73 ( 77.73)
Epoch: [61][ 500/1000]	Time  0.755 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2074e+00 (7.2327e+00)	Acc@1  66.02 ( 64.69)	Acc@5  79.30 ( 78.59)
Epoch: [62][   0/1000]	Time  2.921 ( 2.921)	Data  2.202 ( 2.202)	Loss 7.3018e+00 (7.3018e+00)	Acc@1  59.38 ( 59.38)	Acc@5  73.05 ( 73.05)
Epoch: [62][ 500/1000]	Time  0.754 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1817e+00 (7.2252e+00)	Acc@1  67.97 ( 65.13)	Acc@5  80.47 ( 78.94)
Epoch: [63][   0/1000]	Time  2.931 ( 2.931)	Data  2.336 ( 2.336)	Loss 7.1667e+00 (7.1667e+00)	Acc@1  62.50 ( 62.50)	Acc@5  79.30 ( 79.30)
Epoch: [63][ 500/1000]	Time  0.746 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2290e+00 (7.2220e+00)	Acc@1  64.06 ( 65.25)	Acc@5  78.12 ( 78.91)
Epoch: [64][   0/1000]	Time  2.869 ( 2.869)	Data  2.221 ( 2.221)	Loss 7.2864e+00 (7.2864e+00)	Acc@1  58.20 ( 58.20)	Acc@5  74.22 ( 74.22)
Epoch: [64][ 500/1000]	Time  0.750 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2034e+00 (7.2156e+00)	Acc@1  65.62 ( 65.47)	Acc@5  78.12 ( 79.30)
Epoch: [65][   0/1000]	Time  2.948 ( 2.948)	Data  2.314 ( 2.314)	Loss 7.2359e+00 (7.2359e+00)	Acc@1  64.45 ( 64.45)	Acc@5  78.12 ( 78.12)
Epoch: [65][ 500/1000]	Time  0.755 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2171e+00 (7.2106e+00)	Acc@1  66.80 ( 65.85)	Acc@5  78.12 ( 79.41)
Epoch: [66][   0/1000]	Time  2.832 ( 2.832)	Data  2.220 ( 2.220)	Loss 7.2920e+00 (7.2920e+00)	Acc@1  62.11 ( 62.11)	Acc@5  74.61 ( 74.61)
Epoch: [66][ 500/1000]	Time  0.752 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2209e+00 (7.2011e+00)	Acc@1  64.84 ( 66.27)	Acc@5  82.03 ( 79.70)
Epoch: [67][   0/1000]	Time  2.935 ( 2.935)	Data  2.215 ( 2.215)	Loss 7.1985e+00 (7.1985e+00)	Acc@1  68.36 ( 68.36)	Acc@5  82.81 ( 82.81)
Epoch: [67][ 500/1000]	Time  0.752 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1721e+00 (7.2006e+00)	Acc@1  69.53 ( 66.22)	Acc@5  82.81 ( 79.72)
Epoch: [68][   0/1000]	Time  2.848 ( 2.848)	Data  2.226 ( 2.226)	Loss 7.1937e+00 (7.1937e+00)	Acc@1  63.28 ( 63.28)	Acc@5  80.86 ( 80.86)
Epoch: [68][ 500/1000]	Time  0.753 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1792e+00 (7.1941e+00)	Acc@1  63.67 ( 66.68)	Acc@5  78.52 ( 79.92)
Epoch: [69][   0/1000]	Time  2.912 ( 2.912)	Data  2.217 ( 2.217)	Loss 7.1681e+00 (7.1681e+00)	Acc@1  65.62 ( 65.62)	Acc@5  80.47 ( 80.47)
Epoch: [69][ 500/1000]	Time  0.743 ( 0.755)	Data  0.000 ( 0.005)	Loss 7.1898e+00 (7.1914e+00)	Acc@1  66.02 ( 66.69)	Acc@5  80.47 ( 79.99)
Epoch: [70][   0/1000]	Time  2.903 ( 2.903)	Data  2.201 ( 2.201)	Loss 7.2061e+00 (7.2061e+00)	Acc@1  58.98 ( 58.98)	Acc@5  80.08 ( 80.08)
Epoch: [70][ 500/1000]	Time  0.753 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1637e+00 (7.1891e+00)	Acc@1  63.67 ( 66.69)	Acc@5  78.91 ( 80.09)
Epoch: [71][   0/1000]	Time  2.850 ( 2.850)	Data  2.251 ( 2.251)	Loss 7.1284e+00 (7.1284e+00)	Acc@1  69.14 ( 69.14)	Acc@5  82.03 ( 82.03)
Epoch: [71][ 500/1000]	Time  0.755 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1611e+00 (7.1805e+00)	Acc@1  66.02 ( 67.07)	Acc@5  82.03 ( 80.36)
Epoch: [72][   0/1000]	Time  2.891 ( 2.891)	Data  2.172 ( 2.172)	Loss 7.1782e+00 (7.1782e+00)	Acc@1  64.84 ( 64.84)	Acc@5  80.08 ( 80.08)
Epoch: [72][ 500/1000]	Time  0.751 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1507e+00 (7.1777e+00)	Acc@1  65.62 ( 67.10)	Acc@5  82.42 ( 80.46)
Epoch: [73][   0/1000]	Time  2.911 ( 2.911)	Data  2.231 ( 2.231)	Loss 7.1949e+00 (7.1949e+00)	Acc@1  60.94 ( 60.94)	Acc@5  78.52 ( 78.52)
Epoch: [73][ 500/1000]	Time  0.760 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1110e+00 (7.1753e+00)	Acc@1  69.53 ( 67.28)	Acc@5  80.47 ( 80.44)
Epoch: [74][   0/1000]	Time  2.942 ( 2.942)	Data  2.243 ( 2.243)	Loss 7.2044e+00 (7.2044e+00)	Acc@1  63.67 ( 63.67)	Acc@5  80.86 ( 80.86)
Epoch: [74][ 500/1000]	Time  0.747 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1331e+00 (7.1710e+00)	Acc@1  71.09 ( 67.43)	Acc@5  82.81 ( 80.69)
Epoch: [75][   0/1000]	Time  2.847 ( 2.847)	Data  2.234 ( 2.234)	Loss 7.1543e+00 (7.1543e+00)	Acc@1  62.89 ( 62.89)	Acc@5  81.64 ( 81.64)
Epoch: [75][ 500/1000]	Time  0.755 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.2123e+00 (7.1667e+00)	Acc@1  66.41 ( 67.40)	Acc@5  78.52 ( 80.78)
Epoch: [76][   0/1000]	Time  2.820 ( 2.820)	Data  2.214 ( 2.214)	Loss 7.1238e+00 (7.1238e+00)	Acc@1  64.84 ( 64.84)	Acc@5  84.77 ( 84.77)
Epoch: [76][ 500/1000]	Time  0.748 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1412e+00 (7.1707e+00)	Acc@1  70.70 ( 67.32)	Acc@5  84.38 ( 80.52)
Epoch: [77][   0/1000]	Time  2.958 ( 2.958)	Data  2.342 ( 2.342)	Loss 7.1728e+00 (7.1728e+00)	Acc@1  64.06 ( 64.06)	Acc@5  82.42 ( 82.42)
Epoch: [77][ 500/1000]	Time  0.755 ( 0.755)	Data  0.000 ( 0.005)	Loss 7.1372e+00 (7.1620e+00)	Acc@1  66.02 ( 67.61)	Acc@5  83.20 ( 80.93)
Epoch: [78][   0/1000]	Time  2.854 ( 2.854)	Data  2.252 ( 2.252)	Loss 7.2024e+00 (7.2024e+00)	Acc@1  64.45 ( 64.45)	Acc@5  77.73 ( 77.73)
Epoch: [78][ 500/1000]	Time  0.751 ( 0.755)	Data  0.000 ( 0.005)	Loss 7.1684e+00 (7.1619e+00)	Acc@1  69.92 ( 67.53)	Acc@5  79.69 ( 80.64)
Epoch: [79][   0/1000]	Time  2.904 ( 2.904)	Data  2.290 ( 2.290)	Loss 7.1476e+00 (7.1476e+00)	Acc@1  65.23 ( 65.23)	Acc@5  82.42 ( 82.42)
Epoch: [79][ 500/1000]	Time  0.749 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.0971e+00 (7.1562e+00)	Acc@1  70.31 ( 67.75)	Acc@5  81.64 ( 81.00)
Epoch: [80][   0/1000]	Time  2.917 ( 2.917)	Data  2.202 ( 2.202)	Loss 7.1223e+00 (7.1223e+00)	Acc@1  69.53 ( 69.53)	Acc@5  82.81 ( 82.81)
Epoch: [80][ 500/1000]	Time  0.752 ( 0.755)	Data  0.000 ( 0.005)	Loss 7.1471e+00 (7.1522e+00)	Acc@1  69.53 ( 67.94)	Acc@5  79.69 ( 81.16)
Epoch: [81][   0/1000]	Time  2.827 ( 2.827)	Data  2.225 ( 2.225)	Loss 7.1732e+00 (7.1732e+00)	Acc@1  64.45 ( 64.45)	Acc@5  79.30 ( 79.30)
Epoch: [81][ 500/1000]	Time  0.750 ( 0.755)	Data  0.000 ( 0.005)	Loss 7.1763e+00 (7.1500e+00)	Acc@1  67.19 ( 67.78)	Acc@5  79.30 ( 81.02)
Epoch: [82][   0/1000]	Time  2.843 ( 2.843)	Data  2.230 ( 2.230)	Loss 7.1350e+00 (7.1350e+00)	Acc@1  65.23 ( 65.23)	Acc@5  81.64 ( 81.64)
Epoch: [82][ 500/1000]	Time  0.749 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1540e+00 (7.1466e+00)	Acc@1  71.48 ( 68.06)	Acc@5  82.81 ( 81.13)
Epoch: [83][   0/1000]	Time  2.871 ( 2.871)	Data  2.256 ( 2.256)	Loss 7.1511e+00 (7.1511e+00)	Acc@1  67.19 ( 67.19)	Acc@5  78.91 ( 78.91)
Epoch: [83][ 500/1000]	Time  0.746 ( 0.755)	Data  0.000 ( 0.005)	Loss 7.0565e+00 (7.1456e+00)	Acc@1  70.70 ( 68.06)	Acc@5  83.98 ( 81.10)
Epoch: [84][   0/1000]	Time  2.926 ( 2.926)	Data  2.298 ( 2.298)	Loss 7.1806e+00 (7.1806e+00)	Acc@1  64.06 ( 64.06)	Acc@5  80.47 ( 80.47)
Epoch: [84][ 500/1000]	Time  0.753 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.1315e+00 (7.1401e+00)	Acc@1  65.23 ( 68.22)	Acc@5  79.69 ( 81.26)
Epoch: [85][   0/1000]	Time  2.907 ( 2.907)	Data  2.300 ( 2.300)	Loss 7.1971e+00 (7.1971e+00)	Acc@1  60.55 ( 60.55)	Acc@5  78.12 ( 78.12)
Epoch: [85][ 500/1000]	Time  0.746 ( 0.756)	Data  0.000 ( 0.005)	Loss 7.0615e+00 (7.1387e+00)	Acc@1  77.34 ( 68.13)	Acc@5  87.11 ( 81.27)
