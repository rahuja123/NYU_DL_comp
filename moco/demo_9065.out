Dataset is copied to /tmp
Use GPU: 0 for training
=> creating model 'resnet50'
MoCo(
  (encoder_q): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Sequential(
      (0): Linear(in_features=2048, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=128, bias=True)
    )
  )
  (encoder_k): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Sequential(
      (0): Linear(in_features=2048, out_features=2048, bias=True)
      (1): ReLU()
      (2): Linear(in_features=2048, out_features=128, bias=True)
    )
  )
)
=> loading checkpoint '/scratch/ra3136/checkpoints/moco/moco_unsupervised_0090.pth.tar'
=> loaded checkpoint '/scratch/ra3136/checkpoints/moco/moco_unsupervised_0090.pth.tar' (epoch 91)
Epoch: [91][   0/1000]	Time 10.228 (10.228)	Data  3.051 ( 3.051)	Loss 7.0915e+00 (7.0915e+00)	Acc@1  67.97 ( 67.97)	Acc@5  82.42 ( 82.42)
Epoch: [91][ 500/1000]	Time  0.749 ( 0.781)	Data  0.000 ( 0.103)	Loss 7.1593e+00 (7.1271e+00)	Acc@1  67.97 ( 68.32)	Acc@5  77.73 ( 81.35)
Epoch: [92][   0/1000]	Time  3.028 ( 3.028)	Data  2.397 ( 2.397)	Loss 7.0760e+00 (7.0760e+00)	Acc@1  69.53 ( 69.53)	Acc@5  83.98 ( 83.98)
Epoch: [92][ 500/1000]	Time  0.747 ( 0.752)	Data  0.000 ( 0.006)	Loss 7.1501e+00 (7.1222e+00)	Acc@1  67.19 ( 68.36)	Acc@5  80.86 ( 81.60)
Epoch: [93][   0/1000]	Time  2.980 ( 2.980)	Data  2.372 ( 2.372)	Loss 7.0330e+00 (7.0330e+00)	Acc@1  73.05 ( 73.05)	Acc@5  88.67 ( 88.67)
Epoch: [93][ 500/1000]	Time  0.750 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1013e+00 (7.1221e+00)	Acc@1  70.31 ( 68.50)	Acc@5  82.81 ( 81.48)
Epoch: [94][   0/1000]	Time  3.054 ( 3.054)	Data  2.398 ( 2.398)	Loss 7.1315e+00 (7.1315e+00)	Acc@1  62.89 ( 62.89)	Acc@5  82.81 ( 82.81)
Epoch: [94][ 500/1000]	Time  0.753 ( 0.751)	Data  0.000 ( 0.006)	Loss 7.1610e+00 (7.1197e+00)	Acc@1  68.75 ( 68.43)	Acc@5  81.64 ( 81.43)
Epoch: [95][   0/1000]	Time  3.008 ( 3.008)	Data  2.319 ( 2.319)	Loss 7.0555e+00 (7.0555e+00)	Acc@1  68.36 ( 68.36)	Acc@5  84.38 ( 84.38)
Epoch: [95][ 500/1000]	Time  0.747 ( 0.754)	Data  0.000 ( 0.005)	Loss 7.1095e+00 (7.1207e+00)	Acc@1  69.92 ( 68.36)	Acc@5  80.86 ( 81.39)
Epoch: [96][   0/1000]	Time  2.865 ( 2.865)	Data  2.252 ( 2.252)	Loss 7.1377e+00 (7.1377e+00)	Acc@1  65.62 ( 65.62)	Acc@5  81.64 ( 81.64)
Epoch: [96][ 500/1000]	Time  0.750 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1611e+00 (7.1149e+00)	Acc@1  66.02 ( 68.40)	Acc@5  79.30 ( 81.59)
Epoch: [97][   0/1000]	Time  2.933 ( 2.933)	Data  2.295 ( 2.295)	Loss 7.0774e+00 (7.0774e+00)	Acc@1  63.28 ( 63.28)	Acc@5  83.59 ( 83.59)
Epoch: [97][ 500/1000]	Time  0.747 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1521e+00 (7.1171e+00)	Acc@1  65.23 ( 68.14)	Acc@5  79.30 ( 81.33)
Epoch: [98][   0/1000]	Time  3.034 ( 3.034)	Data  2.412 ( 2.412)	Loss 7.0978e+00 (7.0978e+00)	Acc@1  67.58 ( 67.58)	Acc@5  83.98 ( 83.98)
Epoch: [98][ 500/1000]	Time  0.747 ( 0.753)	Data  0.000 ( 0.006)	Loss 7.1189e+00 (7.1136e+00)	Acc@1  66.80 ( 68.22)	Acc@5  82.03 ( 81.36)
Epoch: [99][   0/1000]	Time  3.070 ( 3.070)	Data  2.438 ( 2.438)	Loss 7.0585e+00 (7.0585e+00)	Acc@1  67.58 ( 67.58)	Acc@5  82.81 ( 82.81)
Epoch: [99][ 500/1000]	Time  0.745 ( 0.751)	Data  0.000 ( 0.006)	Loss 7.1169e+00 (7.1122e+00)	Acc@1  66.80 ( 68.37)	Acc@5  80.86 ( 81.41)
Epoch: [100][   0/1000]	Time  3.099 ( 3.099)	Data  2.460 ( 2.460)	Loss 7.1191e+00 (7.1191e+00)	Acc@1  66.02 ( 66.02)	Acc@5  81.25 ( 81.25)
Epoch: [100][ 500/1000]	Time  0.749 ( 0.749)	Data  0.000 ( 0.006)	Loss 7.0228e+00 (7.1087e+00)	Acc@1  74.61 ( 68.45)	Acc@5  84.77 ( 81.49)
Epoch: [101][   0/1000]	Time  2.924 ( 2.924)	Data  2.330 ( 2.330)	Loss 7.1362e+00 (7.1362e+00)	Acc@1  62.11 ( 62.11)	Acc@5  76.56 ( 76.56)
Epoch: [101][ 500/1000]	Time  0.740 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1448e+00 (7.1096e+00)	Acc@1  64.06 ( 68.50)	Acc@5  80.08 ( 81.41)
Epoch: [102][   0/1000]	Time  2.992 ( 2.992)	Data  2.377 ( 2.377)	Loss 7.1756e+00 (7.1756e+00)	Acc@1  63.28 ( 63.28)	Acc@5  79.69 ( 79.69)
Epoch: [102][ 500/1000]	Time  0.747 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1156e+00 (7.1040e+00)	Acc@1  70.70 ( 68.50)	Acc@5  85.16 ( 81.60)
Epoch: [103][   0/1000]	Time  3.038 ( 3.038)	Data  2.411 ( 2.411)	Loss 7.1328e+00 (7.1328e+00)	Acc@1  61.72 ( 61.72)	Acc@5  79.30 ( 79.30)
Epoch: [103][ 500/1000]	Time  0.756 ( 0.751)	Data  0.000 ( 0.006)	Loss 7.1309e+00 (7.1016e+00)	Acc@1  65.62 ( 68.64)	Acc@5  81.64 ( 81.61)
Epoch: [104][   0/1000]	Time  2.936 ( 2.936)	Data  2.324 ( 2.324)	Loss 7.0685e+00 (7.0685e+00)	Acc@1  69.92 ( 69.92)	Acc@5  83.98 ( 83.98)
Epoch: [104][ 500/1000]	Time  0.751 ( 0.754)	Data  0.000 ( 0.005)	Loss 7.0716e+00 (7.1018e+00)	Acc@1  73.83 ( 68.57)	Acc@5  85.16 ( 81.47)
Epoch: [105][   0/1000]	Time  2.903 ( 2.903)	Data  2.249 ( 2.249)	Loss 7.0679e+00 (7.0679e+00)	Acc@1  68.75 ( 68.75)	Acc@5  83.20 ( 83.20)
Epoch: [105][ 500/1000]	Time  0.747 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0706e+00 (7.0994e+00)	Acc@1  70.70 ( 68.38)	Acc@5  83.98 ( 81.60)
Epoch: [106][   0/1000]	Time  2.897 ( 2.897)	Data  2.293 ( 2.293)	Loss 7.0935e+00 (7.0935e+00)	Acc@1  62.89 ( 62.89)	Acc@5  81.25 ( 81.25)
Epoch: [106][ 500/1000]	Time  0.747 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0861e+00 (7.1010e+00)	Acc@1  74.61 ( 68.21)	Acc@5  84.77 ( 81.45)
Epoch: [107][   0/1000]	Time  2.987 ( 2.987)	Data  2.283 ( 2.283)	Loss 7.1601e+00 (7.1601e+00)	Acc@1  64.84 ( 64.84)	Acc@5  78.12 ( 78.12)
Epoch: [107][ 500/1000]	Time  0.743 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0541e+00 (7.1011e+00)	Acc@1  71.88 ( 68.11)	Acc@5  87.11 ( 81.38)
Epoch: [108][   0/1000]	Time  2.984 ( 2.984)	Data  2.365 ( 2.365)	Loss 7.0968e+00 (7.0968e+00)	Acc@1  66.41 ( 66.41)	Acc@5  82.81 ( 82.81)
Epoch: [108][ 500/1000]	Time  0.752 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1253e+00 (7.0965e+00)	Acc@1  64.06 ( 68.27)	Acc@5  80.47 ( 81.50)
Epoch: [109][   0/1000]	Time  2.862 ( 2.862)	Data  2.230 ( 2.230)	Loss 7.0889e+00 (7.0889e+00)	Acc@1  64.84 ( 64.84)	Acc@5  85.55 ( 85.55)
Epoch: [109][ 500/1000]	Time  0.742 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.1051e+00 (7.0940e+00)	Acc@1  71.09 ( 68.45)	Acc@5  83.59 ( 81.48)
Epoch: [110][   0/1000]	Time  2.896 ( 2.896)	Data  2.218 ( 2.218)	Loss 7.0469e+00 (7.0469e+00)	Acc@1  69.53 ( 69.53)	Acc@5  84.77 ( 84.77)
Epoch: [110][ 500/1000]	Time  0.741 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1609e+00 (7.0942e+00)	Acc@1  63.28 ( 68.37)	Acc@5  78.12 ( 81.47)
Epoch: [111][   0/1000]	Time  2.802 ( 2.802)	Data  2.188 ( 2.188)	Loss 7.0824e+00 (7.0824e+00)	Acc@1  63.67 ( 63.67)	Acc@5  80.47 ( 80.47)
Epoch: [111][ 500/1000]	Time  0.739 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1242e+00 (7.0882e+00)	Acc@1  67.19 ( 68.36)	Acc@5  78.91 ( 81.73)
Epoch: [112][   0/1000]	Time  2.858 ( 2.858)	Data  2.225 ( 2.225)	Loss 7.1374e+00 (7.1374e+00)	Acc@1  63.28 ( 63.28)	Acc@5  77.73 ( 77.73)
Epoch: [112][ 500/1000]	Time  0.745 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.0860e+00 (7.0879e+00)	Acc@1  73.44 ( 68.63)	Acc@5  85.16 ( 81.71)
Epoch: [113][   0/1000]	Time  2.850 ( 2.850)	Data  2.232 ( 2.232)	Loss 7.0924e+00 (7.0924e+00)	Acc@1  67.19 ( 67.19)	Acc@5  82.03 ( 82.03)
Epoch: [113][ 500/1000]	Time  0.742 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0755e+00 (7.0868e+00)	Acc@1  67.19 ( 68.38)	Acc@5  78.91 ( 81.63)
Epoch: [114][   0/1000]	Time  2.917 ( 2.917)	Data  2.229 ( 2.229)	Loss 7.0855e+00 (7.0855e+00)	Acc@1  62.50 ( 62.50)	Acc@5  78.91 ( 78.91)
Epoch: [114][ 500/1000]	Time  0.740 ( 0.749)	Data  0.000 ( 0.005)	Loss 7.1078e+00 (7.0831e+00)	Acc@1  68.75 ( 68.51)	Acc@5  80.86 ( 81.68)
Epoch: [115][   0/1000]	Time  2.882 ( 2.882)	Data  2.252 ( 2.252)	Loss 7.1593e+00 (7.1593e+00)	Acc@1  61.33 ( 61.33)	Acc@5  77.34 ( 77.34)
Epoch: [115][ 500/1000]	Time  0.737 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1087e+00 (7.0875e+00)	Acc@1  64.84 ( 68.09)	Acc@5  78.91 ( 81.40)
Epoch: [116][   0/1000]	Time  2.865 ( 2.865)	Data  2.246 ( 2.246)	Loss 7.0608e+00 (7.0608e+00)	Acc@1  64.84 ( 64.84)	Acc@5  81.25 ( 81.25)
Epoch: [116][ 500/1000]	Time  0.753 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0736e+00 (7.0823e+00)	Acc@1  66.02 ( 68.47)	Acc@5  79.30 ( 81.65)
Epoch: [117][   0/1000]	Time  3.008 ( 3.008)	Data  2.386 ( 2.386)	Loss 7.0620e+00 (7.0620e+00)	Acc@1  66.41 ( 66.41)	Acc@5  79.69 ( 79.69)
Epoch: [117][ 500/1000]	Time  0.748 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1441e+00 (7.0822e+00)	Acc@1  67.19 ( 68.27)	Acc@5  80.47 ( 81.53)
Epoch: [118][   0/1000]	Time  2.983 ( 2.983)	Data  2.363 ( 2.363)	Loss 7.0495e+00 (7.0495e+00)	Acc@1  65.62 ( 65.62)	Acc@5  84.77 ( 84.77)
Epoch: [118][ 500/1000]	Time  0.749 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.0168e+00 (7.0827e+00)	Acc@1  71.48 ( 68.06)	Acc@5  83.59 ( 81.43)
Epoch: [119][   0/1000]	Time  3.060 ( 3.060)	Data  2.436 ( 2.436)	Loss 7.0892e+00 (7.0892e+00)	Acc@1  61.72 ( 61.72)	Acc@5  83.20 ( 83.20)
Epoch: [119][ 500/1000]	Time  0.737 ( 0.750)	Data  0.000 ( 0.006)	Loss 7.0869e+00 (7.0778e+00)	Acc@1  68.75 ( 68.57)	Acc@5  82.42 ( 81.77)
Epoch: [120][   0/1000]	Time  2.980 ( 2.980)	Data  2.366 ( 2.366)	Loss 7.0557e+00 (7.0557e+00)	Acc@1  66.02 ( 66.02)	Acc@5  85.16 ( 85.16)
Epoch: [120][ 500/1000]	Time  0.751 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0958e+00 (7.0795e+00)	Acc@1  66.80 ( 68.30)	Acc@5  80.47 ( 81.60)
Epoch: [121][   0/1000]	Time  2.899 ( 2.899)	Data  2.293 ( 2.293)	Loss 7.0409e+00 (7.0409e+00)	Acc@1  71.09 ( 71.09)	Acc@5  85.94 ( 85.94)
Epoch: [121][ 500/1000]	Time  0.754 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0477e+00 (7.0755e+00)	Acc@1  70.31 ( 68.39)	Acc@5  83.20 ( 81.57)
Epoch: [122][   0/1000]	Time  2.940 ( 2.940)	Data  2.324 ( 2.324)	Loss 7.1799e+00 (7.1799e+00)	Acc@1  56.64 ( 56.64)	Acc@5  75.00 ( 75.00)
Epoch: [122][ 500/1000]	Time  0.751 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0218e+00 (7.0779e+00)	Acc@1  69.92 ( 68.18)	Acc@5  83.98 ( 81.35)
Epoch: [123][   0/1000]	Time  2.954 ( 2.954)	Data  2.285 ( 2.285)	Loss 7.0572e+00 (7.0572e+00)	Acc@1  67.19 ( 67.19)	Acc@5  83.59 ( 83.59)
Epoch: [123][ 500/1000]	Time  0.751 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1027e+00 (7.0744e+00)	Acc@1  72.27 ( 68.30)	Acc@5  80.08 ( 81.52)
Epoch: [124][   0/1000]	Time  2.998 ( 2.998)	Data  2.333 ( 2.333)	Loss 7.0959e+00 (7.0959e+00)	Acc@1  58.59 ( 58.59)	Acc@5  78.91 ( 78.91)
Epoch: [124][ 500/1000]	Time  0.742 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.0400e+00 (7.0728e+00)	Acc@1  70.31 ( 68.42)	Acc@5  84.38 ( 81.72)
Epoch: [125][   0/1000]	Time  3.037 ( 3.037)	Data  2.409 ( 2.409)	Loss 7.0013e+00 (7.0013e+00)	Acc@1  66.80 ( 66.80)	Acc@5  81.64 ( 81.64)
Epoch: [125][ 500/1000]	Time  0.748 ( 0.754)	Data  0.000 ( 0.006)	Loss 7.0778e+00 (7.0726e+00)	Acc@1  69.92 ( 68.16)	Acc@5  80.08 ( 81.56)
Epoch: [126][   0/1000]	Time  3.009 ( 3.009)	Data  2.400 ( 2.400)	Loss 7.1063e+00 (7.1063e+00)	Acc@1  62.89 ( 62.89)	Acc@5  79.69 ( 79.69)
Epoch: [126][ 500/1000]	Time  0.736 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0799e+00 (7.0728e+00)	Acc@1  69.53 ( 68.25)	Acc@5  82.03 ( 81.45)
Epoch: [127][   0/1000]	Time  3.029 ( 3.029)	Data  2.398 ( 2.398)	Loss 7.0099e+00 (7.0099e+00)	Acc@1  69.92 ( 69.92)	Acc@5  85.55 ( 85.55)
Epoch: [127][ 500/1000]	Time  0.742 ( 0.753)	Data  0.000 ( 0.006)	Loss 7.0774e+00 (7.0698e+00)	Acc@1  66.02 ( 68.35)	Acc@5  77.34 ( 81.46)
Epoch: [128][   0/1000]	Time  3.114 ( 3.114)	Data  2.435 ( 2.435)	Loss 7.0585e+00 (7.0585e+00)	Acc@1  66.41 ( 66.41)	Acc@5  83.59 ( 83.59)
Epoch: [128][ 500/1000]	Time  0.752 ( 0.751)	Data  0.000 ( 0.006)	Loss 6.9997e+00 (7.0716e+00)	Acc@1  73.83 ( 67.96)	Acc@5  85.16 ( 81.39)
Epoch: [129][   0/1000]	Time  3.027 ( 3.027)	Data  2.319 ( 2.319)	Loss 7.0868e+00 (7.0868e+00)	Acc@1  67.58 ( 67.58)	Acc@5  78.91 ( 78.91)
Epoch: [129][ 500/1000]	Time  0.751 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0650e+00 (7.0689e+00)	Acc@1  71.09 ( 68.23)	Acc@5  81.25 ( 81.55)
Epoch: [130][   0/1000]	Time  3.018 ( 3.018)	Data  2.400 ( 2.400)	Loss 7.0525e+00 (7.0525e+00)	Acc@1  65.23 ( 65.23)	Acc@5  79.69 ( 79.69)
Epoch: [130][ 500/1000]	Time  0.742 ( 0.752)	Data  0.000 ( 0.006)	Loss 7.1515e+00 (7.0700e+00)	Acc@1  62.89 ( 68.19)	Acc@5  78.52 ( 81.39)
Epoch: [131][   0/1000]	Time  2.946 ( 2.946)	Data  2.351 ( 2.351)	Loss 7.1004e+00 (7.1004e+00)	Acc@1  61.33 ( 61.33)	Acc@5  80.86 ( 80.86)
Epoch: [131][ 500/1000]	Time  0.755 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.1094e+00 (7.0658e+00)	Acc@1  68.75 ( 68.14)	Acc@5  81.64 ( 81.43)
Epoch: [132][   0/1000]	Time  3.063 ( 3.063)	Data  2.376 ( 2.376)	Loss 7.0014e+00 (7.0014e+00)	Acc@1  68.36 ( 68.36)	Acc@5  85.55 ( 85.55)
Epoch: [132][ 500/1000]	Time  0.750 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0471e+00 (7.0656e+00)	Acc@1  64.84 ( 68.20)	Acc@5  80.08 ( 81.52)
Epoch: [133][   0/1000]	Time  3.077 ( 3.077)	Data  2.448 ( 2.448)	Loss 7.1113e+00 (7.1113e+00)	Acc@1  63.67 ( 63.67)	Acc@5  81.64 ( 81.64)
Epoch: [133][ 500/1000]	Time  0.751 ( 0.751)	Data  0.000 ( 0.006)	Loss 7.0234e+00 (7.0636e+00)	Acc@1  71.09 ( 68.17)	Acc@5  81.25 ( 81.50)
Epoch: [134][   0/1000]	Time  3.083 ( 3.083)	Data  2.478 ( 2.478)	Loss 7.1345e+00 (7.1345e+00)	Acc@1  61.72 ( 61.72)	Acc@5  76.95 ( 76.95)
Epoch: [134][ 500/1000]	Time  0.754 ( 0.752)	Data  0.000 ( 0.006)	Loss 6.9967e+00 (7.0657e+00)	Acc@1  72.27 ( 67.95)	Acc@5  86.72 ( 81.24)
Epoch: [135][   0/1000]	Time  3.061 ( 3.061)	Data  2.438 ( 2.438)	Loss 7.0853e+00 (7.0853e+00)	Acc@1  64.84 ( 64.84)	Acc@5  80.47 ( 80.47)
Epoch: [135][ 500/1000]	Time  0.748 ( 0.751)	Data  0.000 ( 0.006)	Loss 7.0427e+00 (7.0601e+00)	Acc@1  69.14 ( 68.45)	Acc@5  80.47 ( 81.68)
Epoch: [136][   0/1000]	Time  2.975 ( 2.975)	Data  2.346 ( 2.346)	Loss 7.1198e+00 (7.1198e+00)	Acc@1  60.94 ( 60.94)	Acc@5  77.73 ( 77.73)
Epoch: [136][ 500/1000]	Time  0.751 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0573e+00 (7.0658e+00)	Acc@1  69.92 ( 67.89)	Acc@5  82.81 ( 81.22)
Epoch: [137][   0/1000]	Time  3.114 ( 3.114)	Data  2.457 ( 2.457)	Loss 7.1349e+00 (7.1349e+00)	Acc@1  60.16 ( 60.16)	Acc@5  78.12 ( 78.12)
Epoch: [137][ 500/1000]	Time  0.746 ( 0.752)	Data  0.000 ( 0.006)	Loss 7.0138e+00 (7.0583e+00)	Acc@1  70.70 ( 68.12)	Acc@5  85.16 ( 81.54)
Epoch: [138][   0/1000]	Time  3.093 ( 3.093)	Data  2.431 ( 2.431)	Loss 7.0515e+00 (7.0515e+00)	Acc@1  66.02 ( 66.02)	Acc@5  83.98 ( 83.98)
Epoch: [138][ 500/1000]	Time  0.753 ( 0.754)	Data  0.000 ( 0.006)	Loss 6.9882e+00 (7.0550e+00)	Acc@1  70.31 ( 68.31)	Acc@5  85.16 ( 81.59)
Epoch: [139][   0/1000]	Time  3.065 ( 3.065)	Data  2.447 ( 2.447)	Loss 7.0016e+00 (7.0016e+00)	Acc@1  68.36 ( 68.36)	Acc@5  85.55 ( 85.55)
Epoch: [139][ 500/1000]	Time  0.732 ( 0.750)	Data  0.000 ( 0.006)	Loss 7.0790e+00 (7.0556e+00)	Acc@1  69.53 ( 68.33)	Acc@5  81.64 ( 81.49)
Epoch: [140][   0/1000]	Time  2.989 ( 2.989)	Data  2.357 ( 2.357)	Loss 7.0372e+00 (7.0372e+00)	Acc@1  67.19 ( 67.19)	Acc@5  80.08 ( 80.08)
Epoch: [140][ 500/1000]	Time  0.745 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.1274e+00 (7.0578e+00)	Acc@1  64.45 ( 67.95)	Acc@5  76.56 ( 81.30)
Epoch: [141][   0/1000]	Time  2.943 ( 2.943)	Data  2.328 ( 2.328)	Loss 7.0324e+00 (7.0324e+00)	Acc@1  65.62 ( 65.62)	Acc@5  81.25 ( 81.25)
Epoch: [141][ 500/1000]	Time  0.746 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.0244e+00 (7.0566e+00)	Acc@1  68.75 ( 68.07)	Acc@5  80.47 ( 81.34)
Epoch: [142][   0/1000]	Time  3.032 ( 3.032)	Data  2.403 ( 2.403)	Loss 7.0457e+00 (7.0457e+00)	Acc@1  64.45 ( 64.45)	Acc@5  84.38 ( 84.38)
Epoch: [142][ 500/1000]	Time  0.752 ( 0.751)	Data  0.000 ( 0.006)	Loss 7.0912e+00 (7.0610e+00)	Acc@1  66.80 ( 67.89)	Acc@5  80.86 ( 81.20)
Epoch: [143][   0/1000]	Time  3.001 ( 3.001)	Data  2.371 ( 2.371)	Loss 7.0610e+00 (7.0610e+00)	Acc@1  67.19 ( 67.19)	Acc@5  84.77 ( 84.77)
Epoch: [143][ 500/1000]	Time  0.739 ( 0.750)	Data  0.000 ( 0.005)	Loss 7.0880e+00 (7.0553e+00)	Acc@1  63.28 ( 68.06)	Acc@5  81.64 ( 81.32)
Epoch: [144][   0/1000]	Time  2.962 ( 2.962)	Data  2.316 ( 2.316)	Loss 7.0325e+00 (7.0325e+00)	Acc@1  68.75 ( 68.75)	Acc@5  84.38 ( 84.38)
Epoch: [144][ 500/1000]	Time  0.741 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0204e+00 (7.0549e+00)	Acc@1  71.48 ( 68.11)	Acc@5  83.98 ( 81.34)
Epoch: [145][   0/1000]	Time  2.994 ( 2.994)	Data  2.385 ( 2.385)	Loss 6.9666e+00 (6.9666e+00)	Acc@1  67.97 ( 67.97)	Acc@5  85.55 ( 85.55)
Epoch: [145][ 500/1000]	Time  0.748 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0826e+00 (7.0516e+00)	Acc@1  63.28 ( 67.95)	Acc@5  81.64 ( 81.48)
Epoch: [146][   0/1000]	Time  2.827 ( 2.827)	Data  2.230 ( 2.230)	Loss 7.0471e+00 (7.0471e+00)	Acc@1  69.92 ( 69.92)	Acc@5  83.20 ( 83.20)
Epoch: [146][ 500/1000]	Time  0.755 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1218e+00 (7.0514e+00)	Acc@1  66.41 ( 67.79)	Acc@5  82.03 ( 81.32)
Epoch: [147][   0/1000]	Time  3.041 ( 3.041)	Data  2.422 ( 2.422)	Loss 7.0757e+00 (7.0757e+00)	Acc@1  69.92 ( 69.92)	Acc@5  82.42 ( 82.42)
Epoch: [147][ 500/1000]	Time  0.749 ( 0.753)	Data  0.000 ( 0.006)	Loss 7.0529e+00 (7.0489e+00)	Acc@1  70.31 ( 68.12)	Acc@5  80.47 ( 81.65)
Epoch: [148][   0/1000]	Time  2.920 ( 2.920)	Data  2.244 ( 2.244)	Loss 7.0590e+00 (7.0590e+00)	Acc@1  62.50 ( 62.50)	Acc@5  79.30 ( 79.30)
Epoch: [148][ 500/1000]	Time  0.747 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.1086e+00 (7.0473e+00)	Acc@1  62.89 ( 68.03)	Acc@5  79.30 ( 81.47)
Epoch: [149][   0/1000]	Time  2.928 ( 2.928)	Data  2.299 ( 2.299)	Loss 7.0367e+00 (7.0367e+00)	Acc@1  64.06 ( 64.06)	Acc@5  81.25 ( 81.25)
Epoch: [149][ 500/1000]	Time  0.750 ( 0.752)	Data  0.001 ( 0.005)	Loss 7.0123e+00 (7.0490e+00)	Acc@1  73.44 ( 68.11)	Acc@5  83.59 ( 81.38)
Epoch: [150][   0/1000]	Time  2.928 ( 2.928)	Data  2.287 ( 2.287)	Loss 7.0869e+00 (7.0869e+00)	Acc@1  63.28 ( 63.28)	Acc@5  78.91 ( 78.91)
Epoch: [150][ 500/1000]	Time  0.748 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0546e+00 (7.0450e+00)	Acc@1  66.02 ( 68.03)	Acc@5  82.03 ( 81.42)
Epoch: [151][   0/1000]	Time  2.877 ( 2.877)	Data  2.249 ( 2.249)	Loss 7.0422e+00 (7.0422e+00)	Acc@1  60.55 ( 60.55)	Acc@5  81.64 ( 81.64)
Epoch: [151][ 500/1000]	Time  0.759 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0427e+00 (7.0434e+00)	Acc@1  66.80 ( 67.98)	Acc@5  80.47 ( 81.52)
Epoch: [152][   0/1000]	Time  2.902 ( 2.902)	Data  2.261 ( 2.261)	Loss 7.1262e+00 (7.1262e+00)	Acc@1  62.11 ( 62.11)	Acc@5  78.91 ( 78.91)
Epoch: [152][ 500/1000]	Time  0.747 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0517e+00 (7.0462e+00)	Acc@1  66.80 ( 68.20)	Acc@5  80.86 ( 81.35)
Epoch: [153][   0/1000]	Time  2.958 ( 2.958)	Data  2.358 ( 2.358)	Loss 7.0243e+00 (7.0243e+00)	Acc@1  62.89 ( 62.89)	Acc@5  83.98 ( 83.98)
Epoch: [153][ 500/1000]	Time  0.752 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.1181e+00 (7.0427e+00)	Acc@1  64.45 ( 68.28)	Acc@5  78.91 ( 81.71)
Epoch: [154][   0/1000]	Time  2.897 ( 2.897)	Data  2.272 ( 2.272)	Loss 7.0271e+00 (7.0271e+00)	Acc@1  65.23 ( 65.23)	Acc@5  83.59 ( 83.59)
Epoch: [154][ 500/1000]	Time  0.747 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0218e+00 (7.0429e+00)	Acc@1  70.70 ( 67.94)	Acc@5  82.03 ( 81.46)
Epoch: [155][   0/1000]	Time  2.858 ( 2.858)	Data  2.231 ( 2.231)	Loss 7.0569e+00 (7.0569e+00)	Acc@1  66.41 ( 66.41)	Acc@5  80.86 ( 80.86)
Epoch: [155][ 500/1000]	Time  0.746 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0904e+00 (7.0423e+00)	Acc@1  65.62 ( 68.06)	Acc@5  80.08 ( 81.37)
Epoch: [156][   0/1000]	Time  2.922 ( 2.922)	Data  2.300 ( 2.300)	Loss 7.0258e+00 (7.0258e+00)	Acc@1  64.45 ( 64.45)	Acc@5  80.86 ( 80.86)
Epoch: [156][ 500/1000]	Time  0.750 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0607e+00 (7.0405e+00)	Acc@1  67.97 ( 68.05)	Acc@5  80.08 ( 81.54)
Epoch: [157][   0/1000]	Time  2.921 ( 2.921)	Data  2.228 ( 2.228)	Loss 6.9947e+00 (6.9947e+00)	Acc@1  64.84 ( 64.84)	Acc@5  80.86 ( 80.86)
Epoch: [157][ 500/1000]	Time  0.747 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0040e+00 (7.0421e+00)	Acc@1  71.48 ( 67.82)	Acc@5  83.98 ( 81.25)
Epoch: [158][   0/1000]	Time  2.915 ( 2.915)	Data  2.251 ( 2.251)	Loss 7.0505e+00 (7.0505e+00)	Acc@1  60.94 ( 60.94)	Acc@5  79.30 ( 79.30)
Epoch: [158][ 500/1000]	Time  0.739 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0450e+00 (7.0371e+00)	Acc@1  66.41 ( 68.24)	Acc@5  77.73 ( 81.61)
Epoch: [159][   0/1000]	Time  2.933 ( 2.933)	Data  2.268 ( 2.268)	Loss 7.1018e+00 (7.1018e+00)	Acc@1  62.89 ( 62.89)	Acc@5  77.73 ( 77.73)
Epoch: [159][ 500/1000]	Time  0.758 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0400e+00 (7.0388e+00)	Acc@1  71.88 ( 67.94)	Acc@5  82.42 ( 81.50)
Epoch: [160][   0/1000]	Time  3.014 ( 3.014)	Data  2.304 ( 2.304)	Loss 7.0103e+00 (7.0103e+00)	Acc@1  69.14 ( 69.14)	Acc@5  83.20 ( 83.20)
Epoch: [160][ 500/1000]	Time  0.753 ( 0.753)	Data  0.000 ( 0.005)	Loss 6.9694e+00 (7.0382e+00)	Acc@1  71.48 ( 68.08)	Acc@5  84.77 ( 81.48)
Epoch: [161][   0/1000]	Time  2.958 ( 2.958)	Data  2.338 ( 2.338)	Loss 7.0565e+00 (7.0565e+00)	Acc@1  67.19 ( 67.19)	Acc@5  80.86 ( 80.86)
Epoch: [161][ 500/1000]	Time  0.752 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0949e+00 (7.0346e+00)	Acc@1  65.62 ( 68.17)	Acc@5  77.73 ( 81.58)
Epoch: [162][   0/1000]	Time  2.872 ( 2.872)	Data  2.228 ( 2.228)	Loss 7.0040e+00 (7.0040e+00)	Acc@1  66.02 ( 66.02)	Acc@5  80.47 ( 80.47)
Epoch: [162][ 500/1000]	Time  0.744 ( 0.752)	Data  0.000 ( 0.005)	Loss 6.9954e+00 (7.0394e+00)	Acc@1  71.88 ( 67.79)	Acc@5  83.59 ( 81.21)
Epoch: [163][   0/1000]	Time  2.987 ( 2.987)	Data  2.255 ( 2.255)	Loss 7.0105e+00 (7.0105e+00)	Acc@1  62.50 ( 62.50)	Acc@5  82.03 ( 82.03)
Epoch: [163][ 500/1000]	Time  0.748 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0556e+00 (7.0375e+00)	Acc@1  69.92 ( 67.99)	Acc@5  79.69 ( 81.36)
Epoch: [164][   0/1000]	Time  2.921 ( 2.921)	Data  2.296 ( 2.296)	Loss 7.0863e+00 (7.0863e+00)	Acc@1  60.94 ( 60.94)	Acc@5  76.56 ( 76.56)
Epoch: [164][ 500/1000]	Time  0.743 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0150e+00 (7.0356e+00)	Acc@1  71.88 ( 68.02)	Acc@5  85.16 ( 81.39)
Epoch: [165][   0/1000]	Time  2.881 ( 2.881)	Data  2.271 ( 2.271)	Loss 7.0331e+00 (7.0331e+00)	Acc@1  67.97 ( 67.97)	Acc@5  82.03 ( 82.03)
Epoch: [165][ 500/1000]	Time  0.746 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0545e+00 (7.0361e+00)	Acc@1  67.19 ( 67.90)	Acc@5  82.03 ( 81.40)
Epoch: [166][   0/1000]	Time  2.892 ( 2.892)	Data  2.272 ( 2.272)	Loss 7.0375e+00 (7.0375e+00)	Acc@1  64.45 ( 64.45)	Acc@5  85.55 ( 85.55)
Epoch: [166][ 500/1000]	Time  0.739 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0402e+00 (7.0351e+00)	Acc@1  67.58 ( 67.87)	Acc@5  80.86 ( 81.38)
Epoch: [167][   0/1000]	Time  2.878 ( 2.878)	Data  2.256 ( 2.256)	Loss 7.0536e+00 (7.0536e+00)	Acc@1  64.06 ( 64.06)	Acc@5  76.56 ( 76.56)
Epoch: [167][ 500/1000]	Time  0.749 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0079e+00 (7.0363e+00)	Acc@1  71.48 ( 67.85)	Acc@5  82.81 ( 81.24)
Epoch: [168][   0/1000]	Time  2.903 ( 2.903)	Data  2.254 ( 2.254)	Loss 7.0130e+00 (7.0130e+00)	Acc@1  64.84 ( 64.84)	Acc@5  83.98 ( 83.98)
Epoch: [168][ 500/1000]	Time  0.757 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.1109e+00 (7.0320e+00)	Acc@1  67.58 ( 67.98)	Acc@5  80.86 ( 81.53)
Epoch: [169][   0/1000]	Time  2.961 ( 2.961)	Data  2.332 ( 2.332)	Loss 7.0669e+00 (7.0669e+00)	Acc@1  64.06 ( 64.06)	Acc@5  78.12 ( 78.12)
Epoch: [169][ 500/1000]	Time  0.745 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0108e+00 (7.0316e+00)	Acc@1  70.31 ( 67.99)	Acc@5  82.42 ( 81.36)
Epoch: [170][   0/1000]	Time  2.855 ( 2.855)	Data  2.222 ( 2.222)	Loss 7.0253e+00 (7.0253e+00)	Acc@1  64.06 ( 64.06)	Acc@5  79.30 ( 79.30)
Epoch: [170][ 500/1000]	Time  0.745 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0417e+00 (7.0293e+00)	Acc@1  71.09 ( 67.96)	Acc@5  82.81 ( 81.51)
Epoch: [171][   0/1000]	Time  2.842 ( 2.842)	Data  2.218 ( 2.218)	Loss 7.0417e+00 (7.0417e+00)	Acc@1  62.50 ( 62.50)	Acc@5  81.25 ( 81.25)
Epoch: [171][ 500/1000]	Time  0.751 ( 0.752)	Data  0.000 ( 0.005)	Loss 6.9681e+00 (7.0296e+00)	Acc@1  69.14 ( 67.97)	Acc@5  82.03 ( 81.37)
Epoch: [172][   0/1000]	Time  2.983 ( 2.983)	Data  2.365 ( 2.365)	Loss 7.0261e+00 (7.0261e+00)	Acc@1  65.62 ( 65.62)	Acc@5  80.08 ( 80.08)
Epoch: [172][ 500/1000]	Time  0.747 ( 0.754)	Data  0.000 ( 0.005)	Loss 6.9621e+00 (7.0338e+00)	Acc@1  74.22 ( 67.61)	Acc@5  85.94 ( 81.17)
Epoch: [173][   0/1000]	Time  2.950 ( 2.950)	Data  2.257 ( 2.257)	Loss 6.9966e+00 (6.9966e+00)	Acc@1  66.02 ( 66.02)	Acc@5  81.25 ( 81.25)
Epoch: [173][ 500/1000]	Time  0.740 ( 0.754)	Data  0.000 ( 0.005)	Loss 7.0930e+00 (7.0302e+00)	Acc@1  67.58 ( 67.86)	Acc@5  80.86 ( 81.40)
Epoch: [174][   0/1000]	Time  2.943 ( 2.943)	Data  2.238 ( 2.238)	Loss 6.9952e+00 (6.9952e+00)	Acc@1  68.36 ( 68.36)	Acc@5  83.98 ( 83.98)
Epoch: [174][ 500/1000]	Time  0.747 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0660e+00 (7.0277e+00)	Acc@1  70.70 ( 68.02)	Acc@5  80.08 ( 81.50)
Epoch: [175][   0/1000]	Time  2.897 ( 2.897)	Data  2.273 ( 2.273)	Loss 7.0495e+00 (7.0495e+00)	Acc@1  58.98 ( 58.98)	Acc@5  76.95 ( 76.95)
Epoch: [175][ 500/1000]	Time  0.747 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0837e+00 (7.0281e+00)	Acc@1  67.58 ( 67.86)	Acc@5  79.69 ( 81.36)
Epoch: [176][   0/1000]	Time  2.921 ( 2.921)	Data  2.312 ( 2.312)	Loss 6.9915e+00 (6.9915e+00)	Acc@1  67.97 ( 67.97)	Acc@5  83.20 ( 83.20)
Epoch: [176][ 500/1000]	Time  0.751 ( 0.752)	Data  0.000 ( 0.005)	Loss 6.9605e+00 (7.0248e+00)	Acc@1  73.83 ( 68.03)	Acc@5  85.94 ( 81.50)
Epoch: [177][   0/1000]	Time  2.901 ( 2.901)	Data  2.285 ( 2.285)	Loss 7.0331e+00 (7.0331e+00)	Acc@1  64.45 ( 64.45)	Acc@5  81.64 ( 81.64)
Epoch: [177][ 500/1000]	Time  0.738 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0066e+00 (7.0268e+00)	Acc@1  67.19 ( 67.79)	Acc@5  80.47 ( 81.37)
Epoch: [178][   0/1000]	Time  2.963 ( 2.963)	Data  2.320 ( 2.320)	Loss 7.0604e+00 (7.0604e+00)	Acc@1  62.11 ( 62.11)	Acc@5  80.86 ( 80.86)
Epoch: [178][ 500/1000]	Time  0.749 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.1303e+00 (7.0238e+00)	Acc@1  62.89 ( 67.84)	Acc@5  76.17 ( 81.39)
Epoch: [179][   0/1000]	Time  2.987 ( 2.987)	Data  2.362 ( 2.362)	Loss 7.1027e+00 (7.1027e+00)	Acc@1  63.28 ( 63.28)	Acc@5  80.08 ( 80.08)
Epoch: [179][ 500/1000]	Time  0.755 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0212e+00 (7.0238e+00)	Acc@1  70.31 ( 67.90)	Acc@5  83.20 ( 81.46)
Epoch: [180][   0/1000]	Time  2.926 ( 2.926)	Data  2.299 ( 2.299)	Loss 7.0278e+00 (7.0278e+00)	Acc@1  58.20 ( 58.20)	Acc@5  82.42 ( 82.42)
Epoch: [180][ 500/1000]	Time  0.751 ( 0.752)	Data  0.000 ( 0.005)	Loss 7.0417e+00 (7.0202e+00)	Acc@1  67.19 ( 68.01)	Acc@5  79.30 ( 81.47)
Epoch: [181][   0/1000]	Time  2.855 ( 2.855)	Data  2.238 ( 2.238)	Loss 6.9504e+00 (6.9504e+00)	Acc@1  65.23 ( 65.23)	Acc@5  83.98 ( 83.98)
Epoch: [181][ 500/1000]	Time  0.754 ( 0.750)	Data  0.000 ( 0.005)	Loss 6.9897e+00 (7.0222e+00)	Acc@1  70.31 ( 67.93)	Acc@5  84.77 ( 81.47)
Epoch: [182][   0/1000]	Time  3.028 ( 3.028)	Data  2.354 ( 2.354)	Loss 7.0932e+00 (7.0932e+00)	Acc@1  58.59 ( 58.59)	Acc@5  76.56 ( 76.56)
Epoch: [182][ 500/1000]	Time  0.750 ( 0.754)	Data  0.000 ( 0.005)	Loss 6.9628e+00 (7.0244e+00)	Acc@1  72.27 ( 67.70)	Acc@5  84.38 ( 81.28)
Epoch: [183][   0/1000]	Time  3.004 ( 3.004)	Data  2.315 ( 2.315)	Loss 6.9784e+00 (6.9784e+00)	Acc@1  71.09 ( 71.09)	Acc@5  83.98 ( 83.98)
Epoch: [183][ 500/1000]	Time  0.750 ( 0.751)	Data  0.000 ( 0.005)	Loss 7.0364e+00 (7.0207e+00)	Acc@1  67.58 ( 67.91)	Acc@5  80.47 ( 81.28)
Epoch: [184][   0/1000]	Time  2.927 ( 2.927)	Data  2.317 ( 2.317)	Loss 6.9843e+00 (6.9843e+00)	Acc@1  60.94 ( 60.94)	Acc@5  78.91 ( 78.91)
Epoch: [184][ 500/1000]	Time  0.746 ( 0.753)	Data  0.000 ( 0.005)	Loss 7.0677e+00 (7.0171e+00)	Acc@1  66.80 ( 68.08)	Acc@5  81.25 ( 81.55)
Epoch: [185][   0/1000]	Time  2.942 ( 2.942)	Data  2.316 ( 2.316)	Loss 6.9821e+00 (6.9821e+00)	Acc@1  66.41 ( 66.41)	Acc@5  83.20 ( 83.20)
Epoch: [185][ 500/1000]	Time  0.754 ( 0.753)	Data  0.000 ( 0.005)	Loss 6.9667e+00 (7.0166e+00)	Acc@1  71.88 ( 68.15)	Acc@5  83.98 ( 81.55)
