+ mkdir /tmp/ra3136
+ export SINGULARITY_CACHEDIR=/tmp/ra3136
+ SINGULARITY_CACHEDIR=/tmp/ra3136
+ cp -rp /scratch/DL21SP/student_dataset.sqsh /tmp
+ echo 'Dataset is copied to /tmp'
+ cd /home/ra3136/NYU_DL_comp/barlow/
/opt/slurm/data/slurmd/job10006/slurm_script: line 22: cd: /home/ra3136/NYU_DL_comp/barlow/: No such file or directory
+ singularity exec --nv --bind /scratch --overlay /scratch/DL21SP/conda.sqsh:ro --overlay /tmp/student_dataset.sqsh:ro /share/apps/images/cuda11.1-cudnn8-devel-ubuntu18.04.sif /bin/bash -c '
source /ext3/env.sh
conda activate dev
CUDA_VISIBLE_DEVICES=0,1 python3  main.py --data /dataset --epochs 200 --batch-size 256 --learning-rate 0.4 --lambd 0.0051 --projector 8192-8192-8192 --scale-loss 0.024 --checkpoint-dir /scratch/ra3136/checkpoints/barlow '
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/2000 [00:00<?, ?it/s][A  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/2000 [00:00<?, ?it/s][A
  0%|          | 1/2000 [00:17<9:28:47, 17.07s/it][A
  0%|          | 1/2000 [00:17<9:28:55, 17.08s/it][A
  0%|          | 2/2000 [00:18<4:19:52,  7.80s/it][A
  0%|          | 2/2000 [00:18<4:19:53,  7.80s/it][A  0%|          | 2/2000 [00:20<5:45:32, 10.38s/it]
  0%|          | 0/200 [00:20<?, ?it/s]
  0%|          | 2/2000 [00:20<5:46:10, 10.40s/it]
  0%|          | 0/200 [00:20<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 328, in <module>
    main()
  File "main.py", line 73, in main
    torch.multiprocessing.spawn(main_worker, (args,), args.ngpus_per_node)
  File "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/home/ra3136/NYU_DL_comp/barlowtwins/main.py", line 129, in main_worker
    scaler.scale(loss).backward()
  File "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/ext3/miniconda3/envs/dev/lib/python3.8/site-packages/torch/autograd/__init__.py", line 145, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 14.76 GiB total capacity; 13.40 GiB already allocated; 23.75 MiB free; 13.69 GiB reserved in total by PyTorch)

